{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 填充和步幅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看一下是如何使用这两个东西的，首先看填充\n",
    "\n",
    "例如，在所有侧边填充一个像素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def comp_conv2d(conv2d,X):\n",
    "    X=X.reshape((1,1)+X.shape)\n",
    "    # 由于这里的X没有考虑通道数和批量大小数，所以这里在X.shape前面加了两个1\n",
    "    # 表示通道数为1，批量大小数为1\n",
    "    Y=conv2d(X)\n",
    "    return Y.reshape(Y.shape[2:])\n",
    "    # 把通道和批量数删除了，只要后面的形状\n",
    "\n",
    "conv2d=nn.Conv2d(1,1,kernel_size=3,padding=1)\n",
    "X=torch.rand(size=(8,8))\n",
    "\n",
    "comp_conv2d(conv2d,X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 注意，这里的padding是指一侧的padding，框架一般都是指定一侧的padding，而沐神课上讲的padding是两层一共的padding，矩阵输入的长/宽实际直接增大的个数\n",
    "+ 所以整体来说，$k_h-2p_h=1$时，卷积层的输入和经过卷积的输出大小不变。没问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然一般都会让填充的上下左右都一样，但是也可以不一样，例如"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d=nn.Conv2d(1,1,kernel_size=(5,3),padding=(2,1))\n",
    "comp_conv2d(conv2d,X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也可以看看步幅，不用自己手动实现，直接调用。。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d=nn.Conv2d(1,1,kernel_size=3,padding=1,stride=2)\n",
    "comp_conv2d(conv2d,X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照之前我在博客里的理解，就是8+2=10,10+（2-3）=9 9/2下取整，就等于4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后可以看一个稍微复杂点的例子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d=nn.Conv2d(1,1,kernel_size=(3,5),padding=(0,1),stride=(3,4))\n",
    "comp_conv2d(conv2d,X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果是这种完全不对称的情况，行和列就要分开计算了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 对于行，8行，行上填充为0，行的kernel为3，行的步幅为3，步幅=kernel，\n",
    "    + 直接8+2*0/3下取整，等于2\n",
    "+ 对于列，同理，只是计算变成了，(8+2\\*1+(4-5))/4=2  (步幅小于卷积核，提前预留二者的差，保证最后一个卷积核可以计算)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> + 一般我们会使用对称的padding和stride以及kernel，因为通常输入的图片是方块。比如28\\*28。256\\*256。一般数据集都是这么提供的。\n",
    "> + 当然，如果是自己采集的另算，如果采集的长宽不一样，那么就可以去控制卷积核，步幅和填充有一定的长宽比的形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
