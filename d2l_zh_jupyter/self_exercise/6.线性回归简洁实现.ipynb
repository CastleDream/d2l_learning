{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简洁实现就是直接调包，不用自己写一些函数实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthetic_data(w,b,num_examples):\n",
    "    X=torch.normal(0,1,(num_examples,len(w)))\n",
    "    y=torch.matmul(X,w)+b\n",
    "    y+=torch.normal(0,0.01,y.shape)\n",
    "    return X,y.reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_w=torch.tensor([2,-3.4])\n",
    "true_b=4.2\n",
    "\n",
    "features, labels=synthetic_data(true_w,true_b,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 调用框架中现有的API来读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.6819, -0.2478],\n",
       "         [ 0.0876, -0.3977],\n",
       "         [ 0.5512,  1.0308],\n",
       "         [-0.7333,  1.5255],\n",
       "         [ 0.2307,  0.7130],\n",
       "         [-0.6225, -0.1732],\n",
       "         [-1.0890, -0.0059],\n",
       "         [-0.5752,  0.3171],\n",
       "         [ 0.3993, -0.1051],\n",
       "         [-1.5982, -0.6899]]),\n",
       " tensor([[ 3.6711],\n",
       "         [ 5.7236],\n",
       "         [ 1.7837],\n",
       "         [-2.4466],\n",
       "         [ 2.2440],\n",
       "         [ 3.5534],\n",
       "         [ 2.0515],\n",
       "         [ 1.9648],\n",
       "         [ 5.3541],\n",
       "         [ 3.3623]])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_array(data_arrays,batch_size,is_train=True):\n",
    "    \"\"\"\n",
    "    构建一个pytorch数据生成器\n",
    "    \"\"\"\n",
    "    dataset=data.TensorDataset(*data_arrays)\n",
    "    print(data_arrays[0][0].size(0),data_arrays[0][23].size(0))\n",
    "    # 第二个是任意一个序号样本 对features和labels分别进行这样的验证\n",
    "    return data.DataLoader(dataset,batch_size,shuffle=is_train)\n",
    "\n",
    "batch_size=10\n",
    "data_iter=load_array((features,labels),batch_size)\n",
    "\n",
    "next(iter(data_iter))\n",
    "# 调用一次next，就会产生一批数据，\n",
    "# 和yield功能差不多，yield内部也实现了next函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "all(features[0].size(0) == tensor.size(0) for tensor in features)\n",
    "print(features[0].size(0))\n",
    "\n",
    "# 整个输入特征的第一维，是每个样本的特征数量，判断每个样本的特征数量是否等于 整个输入特征的第一维\n",
    "# 以第一维 的序号作为这个样本的检索标准"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    *tensors (Tensor): tensors that have the same size of the first dimension.\n",
    "\n",
    "看了一下对应的函数：https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#TensorDataset\n",
    "```python\n",
    "[docs]class TensorDataset(Dataset[Tuple[Tensor, ...]]):\n",
    "    r\"\"\"Dataset wrapping tensors.\n",
    "\n",
    "    Each sample will be retrieved by indexing tensors along the first dimension.\n",
    "\n",
    "    Args:\n",
    "        *tensors (Tensor): tensors that have the same size of the first dimension.\n",
    "    \"\"\"\n",
    "    tensors: Tuple[Tensor, ...]\n",
    "\n",
    "    def __init__(self, *tensors: Tensor) -> None:\n",
    "        assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors), \"Size mismatch between tensors\"\n",
    "        self.tensors = tensors\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return tuple(tensor[index] for tensor in self.tensors)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tensors[0].size(0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最重要的一句话：每个样本将会沿着第一维度的索引被检索。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\software\\anaconda\\lib\\site-packages\\torch\\autograd\\__init__.py:132: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at  ..\\c10\\cuda\\CUDAFunctions.cpp:100.)\n",
      "  allow_unreachable=True)  # allow_unreachable flag\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11.7394, grad_fn=<MseLossBackward>)\n",
      "tensor(21.1278, grad_fn=<MseLossBackward>)\n",
      "tensor(25.6993, grad_fn=<MseLossBackward>)\n",
      "tensor(20.5999, grad_fn=<MseLossBackward>)\n",
      "tensor(14.2759, grad_fn=<MseLossBackward>)\n",
      "tensor(20.4857, grad_fn=<MseLossBackward>)\n",
      "tensor(25.5462, grad_fn=<MseLossBackward>)\n",
      "tensor(6.3983, grad_fn=<MseLossBackward>)\n",
      "tensor(18.1160, grad_fn=<MseLossBackward>)\n",
      "tensor(15.3537, grad_fn=<MseLossBackward>)\n",
      "tensor(14.3052, grad_fn=<MseLossBackward>)\n",
      "tensor(5.8712, grad_fn=<MseLossBackward>)\n",
      "tensor(6.2474, grad_fn=<MseLossBackward>)\n",
      "tensor(5.4770, grad_fn=<MseLossBackward>)\n",
      "tensor(13.1340, grad_fn=<MseLossBackward>)\n",
      "tensor(6.7820, grad_fn=<MseLossBackward>)\n",
      "tensor(5.4515, grad_fn=<MseLossBackward>)\n",
      "tensor(11.0596, grad_fn=<MseLossBackward>)\n",
      "tensor(2.1408, grad_fn=<MseLossBackward>)\n",
      "tensor(4.2195, grad_fn=<MseLossBackward>)\n",
      "tensor(3.8079, grad_fn=<MseLossBackward>)\n",
      "tensor(3.4709, grad_fn=<MseLossBackward>)\n",
      "tensor(2.7187, grad_fn=<MseLossBackward>)\n",
      "tensor(2.2596, grad_fn=<MseLossBackward>)\n",
      "tensor(1.4604, grad_fn=<MseLossBackward>)\n",
      "tensor(2.0891, grad_fn=<MseLossBackward>)\n",
      "tensor(1.2307, grad_fn=<MseLossBackward>)\n",
      "tensor(1.3453, grad_fn=<MseLossBackward>)\n",
      "tensor(1.4313, grad_fn=<MseLossBackward>)\n",
      "tensor(1.0798, grad_fn=<MseLossBackward>)\n",
      "tensor(2.0186, grad_fn=<MseLossBackward>)\n",
      "tensor(0.9477, grad_fn=<MseLossBackward>)\n",
      "tensor(1.6198, grad_fn=<MseLossBackward>)\n",
      "tensor(0.4143, grad_fn=<MseLossBackward>)\n",
      "tensor(1.2889, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3654, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3347, grad_fn=<MseLossBackward>)\n",
      "tensor(0.5368, grad_fn=<MseLossBackward>)\n",
      "tensor(0.7222, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2320, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2963, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1497, grad_fn=<MseLossBackward>)\n",
      "tensor(0.2056, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3012, grad_fn=<MseLossBackward>)\n",
      "tensor(0.3252, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1676, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1211, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0844, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0736, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0788, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1361, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0764, grad_fn=<MseLossBackward>)\n",
      "tensor(0.1326, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0437, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0280, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0347, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0341, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0672, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0262, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0351, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0326, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0247, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0166, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0093, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0231, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0191, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0062, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0125, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0248, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0168, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0030, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0092, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0073, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0034, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0056, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0029, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0031, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0034, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0052, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0012, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0037, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0025, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0022, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0030, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0017, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0009, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0004, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0019, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0010, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0008, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0014, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0006, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "epoch 1,loss 0.000332\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0005, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(7.2451e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(7.1369e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(8.2044e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(5.7027e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(5.9135e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(6.6251e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(5.1710e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(8.5865e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(8.0689e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(1.9731e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(8.0034e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(5.4866e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(9.8845e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(8.0332e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(7.8979e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(5.7127e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0003, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(9.7751e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(8.5095e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(6.8937e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(7.4381e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(8.1212e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(4.7158e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(8.6968e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(7.6693e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(6.4133e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(5.5255e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(4.1976e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(8.7131e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(7.3670e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(8.2612e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(8.6070e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(7.8209e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(7.5702e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(8.5824e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(5.9555e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(6.8143e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(9.2082e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(8.0824e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(4.2945e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(8.8874e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(5.1463e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(9.7784e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(7.5980e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(3.8877e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "epoch 2,loss 0.000100\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(9.3895e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(8.6013e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(6.8035e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(4.3271e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(3.9244e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(6.2959e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(8.2435e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(8.8051e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(6.4714e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(7.7334e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(9.4139e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(9.6088e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(7.6717e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(7.8916e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(5.8831e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(8.8796e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(5.9001e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(8.5859e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(7.5144e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(7.8514e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(7.0568e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(4.5034e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(3.6280e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(7.9072e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(4.9229e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(9.7411e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(9.3410e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(9.5248e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(8.6034e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(9.6024e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(8.4999e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(5.6901e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(3.7837e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(9.5516e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(4.2545e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(3.6166e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(3.2239e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(6.0395e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(2.9130e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(8.2949e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(5.6299e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(3.5519e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(9.1153e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(9.4714e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(3.5207e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(6.9829e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(5.9315e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(5.2291e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(4.6183e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0001, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(0.0002, grad_fn=<MseLossBackward>)\n",
      "tensor(9.5291e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(9.6801e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(5.4596e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(3.5449e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(6.3977e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(4.5155e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(3.6613e-05, grad_fn=<MseLossBackward>)\n",
      "tensor(7.9886e-05, grad_fn=<MseLossBackward>)\n",
      "epoch 3,loss 0.000100\n"
     ]
    }
   ],
   "source": [
    "# 使用框架预定义好的层\n",
    "from torch import nn\n",
    "\n",
    "net=nn.Sequential(nn.Linear(2,1))\n",
    "\n",
    "# 初始化模型参数\n",
    "# print(net[0].weight,net[0].weight.data)\n",
    "# print(net[0].bias,net[0].bias.data)\n",
    "\n",
    "net[0].weight.data.normal_(0,0.01)\n",
    "net[0].bias.data.fill_(0)\n",
    "\n",
    "# pytoch中下划线表示 in-place操作\n",
    "\n",
    "# print(\"\\n\")\n",
    "# print(net[0].weight,net[0].weight.data)\n",
    "# print(net[0].bias,net[0].bias.data)\n",
    "\n",
    "# 权重初始化前后值不同标明，在构建Sequential时，w和b就被赋值了\n",
    "\n",
    "# MSELoss，平方L2范数，就是之前的均方差Mean Square Error loss \n",
    "loss=nn.MSELoss()\n",
    "\n",
    "# 实例化SGD实例\n",
    "trainer=torch.optim.SGD(net.parameters(),lr=0.03)\n",
    "# 需要传入至少两个参数，一是要求梯度下降的超参，一个是学习率\n",
    "\n",
    "num_epochs=3\n",
    "for epoch in range(num_epochs):\n",
    "    for X,y in data_iter:\n",
    "        l=loss(net(X),y)\n",
    "        trainer.zero_grad()\n",
    "        l.backward()\n",
    "        print(l)\n",
    "        trainer.step()\n",
    "    l=loss(net(features),labels)\n",
    "    print(f\"epoch {epoch+1},loss {l:f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "平均平方误差，均方误差MSE mean square error\n",
    "$$cost(w)=\\frac{1}{N}\\sum_{n-1}^N(\\hat{y}_n-y_n)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSEloss官方实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html\n",
    "\n",
    "可以看到在官方实现中，使用默认计算每个样本损失后，进行mean，之前在从头实现里，使用的是sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 优化器.step()\n",
    "\n",
    "    trainer=torch.optim.SGD(net.parameters(),lr=0.03)\n",
    "    trainer.step()\n",
    "\n",
    "https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.step.html#torch.optim.Optimizer.step\n",
    "\n",
    "\"\"\"Performs a single optimization step (parameter update).\"\"\"\n",
    "\n",
    "更新一次梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## linear层定义（官方文档）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    nn.Linear(in_features: int, out_features: int, bias: bool = True) -> None\n",
    "    \n",
    "输入特征维度：2\n",
    "输出特征维度：1\n",
    "\n",
    "线性层就是全连接了，代码：https://pytorch.org/docs/stable/_modules/torch/nn/modules/linear.html\n",
    "\n",
    "主要这里的说明：\n",
    "\n",
    "```python\n",
    "\"\"\"Applies a linear transformation to the incoming data: :math:`y = xA^T + b`\n",
    "\n",
    "    This module supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
    "\n",
    "    Args:\n",
    "        in_features: size of each input sample\n",
    "        out_features: size of each output sample\n",
    "        bias: If set to ``False``, the layer will not learn an additive bias.\n",
    "            Default: ``True``\n",
    "\n",
    "    Shape:\n",
    "        - Input: :math:`(N, *, H_{in})` where :math:`*` means any number of\n",
    "          additional dimensions and :math:`H_{in} = \\text{in\\_features}`\n",
    "        - Output: :math:`(N, *, H_{out})` where all but the last dimension\n",
    "          are the same shape as the input and :math:`H_{out} = \\text{out\\_features}`.\n",
    "```\n",
    "\n",
    "就是应用线性变化，公式就是y=ax+b，输入就是特征维度（features），输出的最后一个维度和输入特征的最后一个维度（即。样本数量）一样。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "关于使用某种分布初始化参数：\n",
    "\n",
    "https://pytorch.org/docs/stable/nn.init.html\n",
    "\n",
    "stack overflow：\n",
    "+ [How to initialize weights in PyTorch?](https://stackoverflow.com/questions/49433936/how-to-initialize-weights-in-pytorch)\n",
    "\n",
    "\n",
    "Single layer\n",
    "To initialize the weights of a single layer, use a function from torch.nn.init. For instance:\n",
    "\n",
    "```\n",
    "conv1 = torch.nn.Conv2d(...)\n",
    "torch.nn.init.xavier_uniform(conv1.weight)\n",
    "```\n",
    "\n",
    "Alternatively, you can modify the parameters by writing to conv1.weight.data (which is a torch.Tensor). Example:\n",
    "\n",
    "```\n",
    "conv1.weight.data.fill_(0.01)\n",
    "# The same applies for biases:\n",
    "conv1.bias.data.fill_(0.01)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
