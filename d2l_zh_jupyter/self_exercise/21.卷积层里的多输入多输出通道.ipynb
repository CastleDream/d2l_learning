{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.bilibili.com/video/BV1MB4y1F7of?p=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多输入多输出通道"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现一下多输入输出通道互相关运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  多输入通道的互相关运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from d2l_torch import d2l\n",
    "\n",
    "# 如果是mac要重新规定一下线程数量\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_multi_in(X,K):\n",
    "    \"\"\"\n",
    "    多输入二维卷积的计算实现\n",
    "    X 表示3d的输入（每次拿出一个通道的矩阵来）\n",
    "    K 表示3d的卷积核(每次拿出一个卷积核来)\n",
    "    \"\"\"\n",
    "    return sum(d2l.corr2d(x,k) for x,k in zip(X,K))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://img-blog.csdnimg.cn/cfc3c3143919489f9e4ab7de7939175b.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  72.],\n",
       "        [104., 120.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 比如，以上图中的输入和核来验证一下我们刚刚写的多输入通道函数的计算\n",
    "X=torch.tensor([\n",
    "                [[0.,1.,2.],[3.,4.,5.],[6.,7.,8.]],\n",
    "                [[1.,2.,3.],[4.,5.,6.],[7.,8.,9.]]\n",
    "               ])\n",
    "K=torch.tensor([[[0.,1.],[2.,3.]],\n",
    "                [[1.,2.],[3.,4.]]\n",
    "               ])\n",
    "corr2d_multi_in(X,K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多输出通道的互相关运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_multi_out(X,K):\n",
    "    \"\"\"\n",
    "    计算多输出通道的互相关运算\n",
    "    X表示输入，是一个3d的张量\n",
    "    K表示卷积核，多输出通道的卷积核是一个4d张量，(c_o,c_i,k_h,k_w) \n",
    "    \"\"\"\n",
    "    return torch.stack([corr2d_multi_in(X,k) for k in K],dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 2, 2])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K=torch.tensor([[[0.,1.],[2.,3.]],\n",
    "                [[1.,2.],[3.,4.]]\n",
    "               ])\n",
    "K=torch.stack((K,K+1,K+2),0)\n",
    "K.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 56.,  72.],\n",
       "         [104., 120.]],\n",
       "\n",
       "        [[ 76., 100.],\n",
       "         [148., 172.]],\n",
       "\n",
       "        [[ 96., 128.],\n",
       "         [192., 224.]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr2d_multi_out(X,K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 多输入通道中，包含两个$2\\times 2$的卷积核，输出是$2\\times 2$\n",
    "+ 多输出通道中，输出通道为3，包含3组卷积核，其中每组卷积核都包含两个$2\\times 2$的卷积核，输出是$3\\times 2\\times 2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1X1的卷积等价于全连接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y1=tensor([[[ 0.8397,  0.4864,  0.3800],\n",
      "         [ 0.1702,  0.0867,  0.1117],\n",
      "         [ 0.1599, -0.2185, -0.4287]],\n",
      "\n",
      "        [[-0.2285, -1.5198, -4.3252],\n",
      "         [-5.8408, -0.7174,  1.3526],\n",
      "         [ 1.2860, -0.2413, -0.9581]]]), \n",
      " Y2=tensor([[[ 0.8397,  0.4864,  0.3800],\n",
      "         [ 0.1702,  0.0867,  0.1117],\n",
      "         [ 0.1599, -0.2185, -0.4287]],\n",
      "\n",
      "        [[-0.2285, -1.5198, -4.3252],\n",
      "         [-5.8408, -0.7174,  1.3526],\n",
      "         [ 1.2860, -0.2413, -0.9581]]])\n"
     ]
    }
   ],
   "source": [
    "def corr2d_multi_out_1x1(X,K):\n",
    "    c_i,h,w=X.shape\n",
    "    c_o=K.shape[0]\n",
    "    X=X.reshape(c_i,h*w)\n",
    "    # 转换成全连接，c_i个输入，每个输入是h*w维度\n",
    "    K=K.reshape((c_o,c_i))\n",
    "    # 因为后面两维其实是(1,1)\n",
    "    Y=torch.matmul(K,X)\n",
    "    # 这里没有进行加偏置\n",
    "    return Y.reshape((c_o,h,w))\n",
    "\n",
    "X=torch.normal(0,1,(3,3,3))\n",
    "# 输入是三通道，每个通道是(3,3)\n",
    "K=torch.normal(0,1,(2,3,1,1))\n",
    "# 卷积核是两组，每组有3个(1,1)的卷积核\n",
    "\n",
    "Y1=corr2d_multi_out_1x1(X,K)\n",
    "Y2=corr2d_multi_out(X,K)\n",
    "\n",
    "print(f\"Y1={Y1}, \\n Y2={Y2}\")\n",
    "\n",
    "assert float(torch.abs(Y1-Y2).sum())<1e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  torch的和numpy的stack，cat（concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 2, 2]),\n",
       " torch.Size([3, 2, 2, 2]),\n",
       " tensor([[[[0., 1.],\n",
       "           [2., 3.]],\n",
       " \n",
       "          [[1., 2.],\n",
       "           [3., 4.]]],\n",
       " \n",
       " \n",
       "         [[[1., 2.],\n",
       "           [3., 4.]],\n",
       " \n",
       "          [[2., 3.],\n",
       "           [4., 5.]]],\n",
       " \n",
       " \n",
       "         [[[2., 3.],\n",
       "           [4., 5.]],\n",
       " \n",
       "          [[3., 4.],\n",
       "           [5., 6.]]]]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K=torch.tensor([[[0.,1.],[2.,3.]],\n",
    "                [[1.,2.],[3.,4.]]\n",
    "               ])\n",
    "a=torch.stack((K,K+1,K+2),0)\n",
    "K.shape,a.shape,a\n",
    "# 所以K，K+1，K+2stack构成a时，只需要把这三个元素放到一起，然后最外层套一层/一个轴就可以"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0., 1.],\n",
       "          [2., 3.]],\n",
       " \n",
       "         [[1., 2.],\n",
       "          [3., 4.]]]),\n",
       " tensor([[[-1.,  0.],\n",
       "          [ 1.,  2.]],\n",
       " \n",
       "         [[ 0.,  1.],\n",
       "          [ 2.,  3.]]]),\n",
       " tensor([[[-2., -1.],\n",
       "          [ 0.,  1.]],\n",
       " \n",
       "         [[-1.,  0.],\n",
       "          [ 1.,  2.]]]),\n",
       " array([[[[ 0., -1., -2.],\n",
       "          [ 1.,  0., -1.]],\n",
       " \n",
       "         [[ 2.,  1.,  0.],\n",
       "          [ 3.,  2.,  1.]]],\n",
       " \n",
       " \n",
       "        [[[ 1.,  0., -1.],\n",
       "          [ 2.,  1.,  0.]],\n",
       " \n",
       "         [[ 3.,  2.,  1.],\n",
       "          [ 4.,  3.,  2.]]]], dtype=float32),\n",
       " (2, 2, 2, 3))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K=torch.tensor([[[0.,1.],[2.,3.]],\n",
    "                [[1.,2.],[3.,4.]]\n",
    "               ])\n",
    "\n",
    "a=np.stack((K,K-1,K-2),-1)\n",
    "\n",
    "K,K-1,K-2,a,a.shape\n",
    "# 如果是-1，则最里面那层是新加的轴。\n",
    "# 从结果不难发现，其实就是K，K-1，K-2中每个位置的元素，取一个。\n",
    "# 比如，stack后的结果最内层的第一个元素是[0,-1,-2]。其实就分别对应于K，K-1，K-2中的[0,0,0]号元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2, 2, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K=torch.tensor([[[0.,1.],[2.,3.]],\n",
    "                [[1.,2.],[3.,4.]]\n",
    "               ])\n",
    "K=np.stack((K,K+1,K+2),0)\n",
    "K.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看上面的代码，其实\n",
    "```python\n",
    "K=torch.tensor([[[0.,1.],[2.,3.]],\n",
    "                [[1.,2.],[3.,4.]]\n",
    "               ])\n",
    "\n",
    "torch.stack()\n",
    "Concatenates a sequence of tensors along a new dimension.\n",
    "All tensors need to be of the same size.\n",
    "\n",
    "\n",
    "numpy.stack()\n",
    "Join a sequence of arrays along a new axis.\n",
    "\n",
    "The ``axis`` parameter specifies the index of the new axis in the\n",
    "dimensions of the result. For example, if ``axis=0`` it will be the first\n",
    "dimension and if ``axis=-1`` it will be the last dimension.\n",
    "\n",
    "```\n",
    "\n",
    "+ 所以这里对三个K进行stack，这里torch的stack和numpy的stack函数其实一模一样，如果轴dim=0，就在最前面加一个轴，如果=-1，就在最后面加一个轴\n",
    "+ 容易搞混的是，np.hstack(),np.vstack()这两个一个是水平方向拼接，一个是竖直方向拼接，常见于二维数据的操作，比如dataframe。torch也有这几个函数\n",
    "+ 所以torch和numpy的函数基本一模一样，区别就是一个在cuda上实现过！另一个没有\n",
    "\n",
    "---\n",
    "\n",
    "另外，torch中对应于stack，其实也有concat，但是一般常写作为cat,如下示例\n",
    "+ concat不同于stack，不会增加新的轴，只会在某个指定的轴上对元素进行拼接\n",
    "+ torch中的cat功能和numpy中的concat一模一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 2, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K=torch.tensor([[[0.,1.],[2.,3.]],\n",
    "                [[1.,2.],[3.,4.]]\n",
    "               ])\n",
    "a= torch.cat((K,K+1,K+2),0)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= torch.cat((K,K+1,K+2),1)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 6])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a= torch.cat((K,K+1,K+2),2)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
