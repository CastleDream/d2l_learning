{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.bilibili.com/video/BV1AK4y1P7vs?p=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 层和块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 两层的多层感知机"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，回顾一下多层感知机"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0377,  0.1395,  0.2692, -0.2100, -0.0620, -0.2493,  0.2391,  0.0307,\n",
       "         -0.1823, -0.0657],\n",
       "        [ 0.1158,  0.1317,  0.2809, -0.2197,  0.0016, -0.2659,  0.1813,  0.1571,\n",
       "         -0.2954, -0.0216]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "# 这是一个两层的感知机，\n",
    "net =nn.Sequential(nn.Linear(20,256),nn.ReLU(),nn.Linear(256,10))\n",
    "\n",
    "# 随机生成输入数据  2是批量大小，现在数据一共就够1个批次，20是数据维度\n",
    "X=torch.rand(2,20)\n",
    "\n",
    "net(X)\n",
    "# 输出就是2*10，因为是两个样本，每个样本输出是10维"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 这里值得注意的是，`nn.Sequential`定义了一种特殊的`Module`。\n",
    "+ 任何一个神经网络的层都应该是`Module`的子类\n",
    "+ 不管是一层，还是多层的模型，都应该是`Module`的子类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 继承Module模块来实现MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以通过继承`Module`来自定义一些层/块\n",
    "+ 最重要的需要实现的是两个函数，一个就是`__init__`函数，里面放着要定义的层的情况\n",
    "+ 另一个就是forward函数，这个层的计算过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden=nn.Linear(20,256)\n",
    "        self.out=nn.Linear(256,10)\n",
    "        \n",
    "    def forward(self,X):\n",
    "        return self.out(F.relu(self.hidden(X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "dir(F)\n",
    "help(F)\n",
    "```\n",
    "可以用上面的帮助看看，torch.nn.Functional里有很多激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实例化多层感知机的层，然后在每次调用正向传播函数时调用这些层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0030, -0.1093, -0.0364, -0.1911, -0.0560, -0.1290, -0.0835, -0.1231,\n",
       "         -0.0680,  0.1367],\n",
       "        [-0.2054, -0.1313, -0.0451, -0.1426, -0.0807,  0.0965, -0.1160, -0.1647,\n",
       "         -0.1683,  0.2031]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net=MLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 顺序块（实现nn.Sequential）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以用一个函数实现nn.Sequential的功能\n",
    "+ 其中`*args`表示一个list of arguments，在上面的`nn.Sequential`调用时，传入的参数是一个网络层的列表，所以下面的*args其实也是一个网络层的列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0631, -0.0432,  0.0557, -0.1294, -0.0249,  0.2181,  0.1399, -0.1292,\n",
       "          0.1484,  0.1344],\n",
       "        [ 0.1204, -0.0275,  0.1475, -0.1368, -0.0464,  0.1692,  0.1496, -0.2167,\n",
       "          0.0604,  0.0680]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MySequential(nn.Module):\n",
    "    def __init__(self,*args):\n",
    "        super().__init__()\n",
    "        for block in args:\n",
    "            self._modules[block]=block\n",
    "            # 这是一个ordered dict 是有序的\n",
    "    \n",
    "    def forward(self,X):\n",
    "        for block in self._modules.values():\n",
    "            X=block(X)\n",
    "        return X\n",
    "    \n",
    "    \n",
    "net=MySequential(nn.Linear(20,256),nn.ReLU(),nn.Linear(256,10))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看一下关于nn.Module.modules的一些说明，其实作用和上面的那个`self._modules`感觉差不多"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function modules in module torch.nn.modules.module:\n",
      "\n",
      "modules(self) -> Iterator[ForwardRef('Module')]\n",
      "    Returns an iterator over all modules in the network.\n",
      "    \n",
      "    Yields:\n",
      "        Module: a module in the network\n",
      "    \n",
      "    Note:\n",
      "        Duplicate modules are returned only once. In the following\n",
      "        example, ``l`` will be returned only once.\n",
      "    \n",
      "    Example::\n",
      "    \n",
      "        >>> l = nn.Linear(2, 2)\n",
      "        >>> net = nn.Sequential(l, l)\n",
      "        >>> for idx, m in enumerate(net.modules()):\n",
      "                print(idx, '->', m)\n",
      "    \n",
      "        0 -> Sequential(\n",
      "          (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "          (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "        )\n",
      "        1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nn.Module.modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在正向传播函数中执行代码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 如果Sequential不能满足你的需求的话，可以继承`nn.Module`来构建自己的网络\n",
    "+ 主要的好处是在`__init__`和`forward`函数中，可以做大量自定义计算\n",
    "+ 下面这个例子没有任何意义，只是为了表达，使用继承这种方式自定义的网络，可以进行非常高程度的定制。比如可以让某层的权重不参与训练，类似固定权重，预训练模型，前面的固定，只修改后几层的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.0077, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FixedHiddenMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.rand_weight=torch.rand((20,20),requires_grad=False)\n",
    "        # 这里注意，这个权重是不参与训练的\n",
    "        self.linear=nn.Linear(20,20)     \n",
    "    \n",
    "    def forward(self,X):\n",
    "        X=self.linear(X)\n",
    "        X=F.relu(torch.mm(X,self.rand_weight)+1)\n",
    "        X=self.linear(X)\n",
    "        while X.abs().sum()>1:\n",
    "            X/=2\n",
    "        return X.sum()\n",
    "\n",
    "net=FixedHiddenMLP()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 使用继承的这种方式，可以比使用Sequential方式更好满足自己的需求\n",
    "+ 反向计算是不需要定义的，只要定义好前向，反向就可以根据前向自动求导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 混合搭配各种组合块的方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 不管是自己继承`nn.Module`得到的类，还是使用nn.Sequential得到的网络，其实都是nn.Module的子类，所以可以混合着用。\n",
    "+ 底层是同一套逻辑\n",
    "+ 网络层可以随便组，随便嵌套，疯狂套娃"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.3004, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net=nn.Sequential(nn.Linear(20,64),nn.ReLU(),\n",
    "                              nn.Linear(64,32),nn.ReLU())\n",
    "        self.linear=nn.Linear(32,16)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        return self.linear(self.net(X))\n",
    "\n",
    "chimera=nn.Sequential(NestMLP(),nn.Linear(16,20),FixedHiddenMLP())\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "https://www.bilibili.com/video/BV1AK4y1P7vs?p=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参数管理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 假设已经定义好类了/模型结构了，该如何去访问参数\n",
    "+ 还是以 具有单隐藏层的 多层感知机 为例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2548],\n",
       "        [-0.2890]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "net=nn.Sequential(nn.Linear(4,8),nn.ReLU(),nn.Linear(8,1))\n",
    "X=torch.rand(size=(2,4))\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数访问"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 把每层的权重都拿出来"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[ 0.0675,  0.2736,  0.0428,  0.1020, -0.1843,  0.0543, -0.1426, -0.1512]])),\n",
       "             ('bias', tensor([-0.1674]))])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 这里net是使用`nn.Sequential`定义的，sequential其实就是类似list的一个东西，可以认为sequential就是python的list\n",
    "+ 从自动机的角度来讲，每个层的权重就是每个层的状态，是会变的，所以这里访问参数和偏置的时候，调用的是`state_dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=8, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然也可以直接访问特定的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0675,  0.2736,  0.0428,  0.1020, -0.1843,  0.0543, -0.1426, -0.1512]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([-0.1674], requires_grad=True)\n",
      "tensor([-0.1674])\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(type(net[2].bias))\n",
    "print(net[2].bias)\n",
    "print(net[2].bias.data)\n",
    "print(net[2].bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以直接调用下面的命令，发现其实有bias和weight属性\n",
    "```python\n",
    "dir(net[2])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[2].weight.grad==None\n",
    "\n",
    "# 由于这里还没有进行反向计算，所以grad为None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 也可以一次性访问所有参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n"
     ]
    }
   ],
   "source": [
    "print(*[(name,param.shape) for name,param in net[0].named_parameters()])\n",
    "print(*[(name,param.shape) for name,param in net.named_parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 这里就会给出参数的名称，和对应参数的形状。\n",
    "+ 如果在定义这些参数的时候，没有显示指定名称，则就默认 `#num.weight/bias`，层的名称加上是权重还是偏置，就可以了\n",
    "+ ReLU，也就是序号为1的层，没有权重，所以没有参数，取不出来东西\n",
    "+ 另外，有了名字就可以用名字来获取参数的信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1674])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()[\"2.bias\"].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('weight', torch.Size([8, 4])), ('bias', torch.Size([8]))]\n",
      "[('0.weight', torch.Size([8, 4])), ('0.bias', torch.Size([8])), ('2.weight', torch.Size([1, 8])), ('2.bias', torch.Size([1]))]\n"
     ]
    }
   ],
   "source": [
    "print([(name,param.shape) for name,param in net[0].named_parameters()])\n",
    "print([(name,param.shape) for name,param in net.named_parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，这部分代码没有前面的`*`号也可以运行，只是输出结果会是列表，而不是上面的元组。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从嵌套块收集参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6426],\n",
       "        [0.6423]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4,8),nn.ReLU(),nn.Linear(8,4),nn.ReLU())\n",
    "\n",
    "def block2():\n",
    "    net =nn.Sequential()\n",
    "    for i in range(4):\n",
    "        net.add_module(f'block {i}',block1())\n",
    "        # 第一个参数是name，第二个参数是网络层module\n",
    "    return net\n",
    "\n",
    "rgnet=nn.Sequential(block2(),nn.Linear(4,1))\n",
    "rgnet(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (block 0): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (block 1): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (block 2): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (block 3): Sequential(\n",
       "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 可以看到，嵌套时候的网络表示，稍微复杂了一点，也嵌套了\n",
    "+ 这个时候如果想从嵌套块收集参数，就要像json嵌套访问一样，一层一层去看\n",
    "+ 最外层是 0→block2,1→linear\n",
    "+ 0之后就是block2\n",
    "+ 注意，这里只支持数字访问，不支持module的name的str访问"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (3): ReLU()\n",
      ")\n",
      "Linear(in_features=4, out_features=8, bias=True)\n"
     ]
    }
   ],
   "source": [
    "print(rgnet[0][0])\n",
    "print(rgnet[0][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 模型比较复杂的时候，看起来就会比较费劲\n",
    "+ 所以需要设计好模块化的东西，这样对自己会比较方便"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1674])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()[\"2.bias\"].data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7:50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
