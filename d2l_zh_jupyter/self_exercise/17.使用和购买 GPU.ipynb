{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.bilibili.com/video/BV1z5411c7C1?p=1\n",
    "\n",
    "先看下有没有GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec  1 08:47:36 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 462.59       Driver Version: 462.59       CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce GTX 165... WDDM  | 00000000:02:00.0 Off |                  N/A |\n",
      "| N/A   40C    P8     2W /  N/A |    194MiB /  4096MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1860    C+G   Insufficient Permissions        N/A      |\n",
      "|    0   N/A  N/A      3704    C+G   ...2\\extracted\\WeChatApp.exe    N/A      |\n",
      "|    0   N/A  N/A      6568    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      9616    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     10444    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\n",
      "|    0   N/A  N/A     10940    C+G   ...nputApp\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     11192    C+G   ...8wekyb3d8bbwe\\Cortana.exe    N/A      |\n",
      "|    0   N/A  N/A     14048    C+G   ...tracted\\WechatBrowser.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算设备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所有的框架，默认都是在cpu上计算的，所以如果想使用gpu，必须提前先指定一下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "<torch.cuda.device object at 0x00000212CFF87348>\n",
      "<torch.cuda.device object at 0x00000212CFF87E08>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# 指定使用cpu作为计算设备（也是默认选项）\n",
    "print(torch.device('cpu'))\n",
    "\n",
    "# 这里torch使用cuda代替gpu，但是cuda和gpu不是那么的等价，\n",
    "# 所以表达上其实存在点问题，但是没什么大的影响，就继续沿用了\n",
    "print(torch.cuda.device('cuda')) # 没有指明数字，默认就是第0个gpu\n",
    "\n",
    "\n",
    "# 多个gpu的话，指定数字，如第1个gpu\n",
    "print(torch.cuda.device('cuda:1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果要指定多个GPU的话，比如单机多卡训练，这时候，写法就不太一样，下面这种是错的    \n",
    "```python   \n",
    "print(torch.cuda.device('cuda:[0,1]'))\n",
    "``` \n",
    "参考：[pytorch指定用多张显卡训练_pytorch多gpu并行训练](https://blog.csdn.net/weixin_42291186/article/details/112207044)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查询可用gpu的数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这两个函数允许我们在请求的GPU不存在的情况下运行代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cuda', index=0),\n",
       " device(type='cuda', index=10),\n",
       " [device(type='cuda', index=0)])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def try_gpu(i=0):\n",
    "    \"\"\"如果存在，则返回gpu(i)，否则返回cpu()\"\"\"\n",
    "    if torch.cuda.device_count()>=1:\n",
    "        return torch.device(f\"cuda:{i}\")\n",
    "    return torch.device(\"cpu\")\n",
    "\n",
    "def try_all_gpus():\n",
    "    \"\"\"返回所有可用的GPU，如果没有GPU，则返回[cpu(),]\"\"\"\n",
    "    gpu_num=torch.cuda.device_count()\n",
    "    devices=[torch.device(f\"cuda:{i}\") for i in range(gpu_num)]\n",
    "    return devices if devices else [torch.device('cpu')]\n",
    "\n",
    "try_gpu(),try_gpu(10),try_all_gpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里显示的内容和李沐老师的不一样\n",
    "+ windows下，其实我的gpu只有1个，index应该只会为0\n",
    "+ 但是在`try_gpu(10)`中，还是返回`index=10`，说明程序里设备序号和实际的物理设备序号有一层映射，不同的框架下，这个映射规定有所不同，不要太纠结。用的时候注意点就行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 张量所在的设备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=torch.tensor([1,2,3])\n",
    "x.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 可以使用`device`查询当前张量所在的设备，默认就是在cpu\n",
    "+ 但是可以在创建的时候指定，放到gpu上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=torch.ones(2,3,device=try_gpu())\n",
    "X.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个的执行速度明显要比在cpu上慢一点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B=torch.ones(2,3,device=torch.device(\"cuda:0\"))\n",
    "B.device\n",
    "# 也可以直接指定设备，没必要写那个函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里有一个问题需要注意：\n",
    "+ 默认张量是创建在cpu上，所以两个张量计算，也是在cpu上进行\n",
    "+ 如果张量创建在了gpu上，那么张量计算也是发生在gpu上，但是需要保证，两个张量位于一块GPU。。有多卡的人需要注意这一点\n",
    "+ 假设X是在第一张GPU卡上创建的张量，Y是第二张GPU，那么如果需要计算，需要先将其中一个张量移动到另一个张量所在的卡上\n",
    "\n",
    "```python\n",
    "Z=X.cuda(1)\n",
    "# 表示将X的值从一个gpu0复制到gpu1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 实际上，张量放在不同的GPU上，也可以移动之后再去计算。\n",
    "+ 但是对于CPU和GPU混用的机器来说，由于数据在CPU和GPU之间交换速度很慢，所以考虑到这点，一旦某个网络有一部分在CPU，有一部分在GPU，就直接给你报个错，因为这样很难保证性能\n",
    "+ 应该是底层实现不容易，同时利用cpu和gpu的资源很难实现的很好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络和GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9463],\n",
       "        [0.9463]], device='cuda:0', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net=nn.Sequential(nn.Linear(3,1))\n",
    "net=net.to(device=try_gpu()) # Moves and/or casts the parameters and buffers.\n",
    "\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里注意一个区别：\n",
    "+ torch.device(\"cuda:1\")是直接指定设备\n",
    "+ net.to()这个函数是将参数等移动到/映射到另一个地方"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "确认模型参数存储在同一个GPU上"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
