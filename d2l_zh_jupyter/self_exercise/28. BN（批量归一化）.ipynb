{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 代码slide：https://courses.d2l.ai/zh-v2/assets/notebooks/chapter_convolutional-modern/batch-norm.slides.html#/\n",
    "+ 视频地址： https://www.bilibili.com/video/BV1X44y1r77r?p=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从0实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 想实现batch_normalization这个层，首先要实现batch_norm这个操作，也就是那个公式计算\n",
    "+ 类似当时实现卷积层的时候，先实现了二维互相关运算，再去实现卷积层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import d2l_torch as d2l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实现batch_norm计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ github的issue：[Acquiring \"is_grad_enabled()\" inside an autograd function #56370](https://github.com/pytorch/pytorch/issues/56370)\n",
    "+ pytorch文档：[TORCH.IS_GRAD_ENABLED](https://pytorch.org/docs/stable/generated/torch.is_grad_enabled.html?highlight=is_grad_enabled)\n",
    "> Returns True if grad mode is currently enabled.   返回当前求导模式是否可用。\n",
    "\n",
    "----\n",
    "另外，搜索过程中发现更常见的一个语句是：`torch.set_grad_enabled(False)`\n",
    "参考：[torch.set_grad_enabled(False)](https://blog.csdn.net/qq_40840797/article/details/119734575)\n",
    "+ volatile是用来在验证或者测试的时候将输入设置成volatile，这样后继几点全部都被设置成volatile，也就是不需要自动求导。\n",
    "+ 0.4.0以后采用torch.set_grad_enabled()来替代这种用法\n",
    "+ 但是torch.set_grad_enabled()在使用的时候是设置一个上下文环境，也就是说只要设置了torch.set_grad_enabled(False)那么接下来所有的tensor运算产生的新的节点都是不可求导的，这个相当于一个全局的环境，即使是多个循环或者是在函数内设置的调用，只要torch.set_grad_enabled(False)出现，则不管是在下一个循环里还是在主函数中，都不再求导，除非单独设置一个孤立节点，并把他的requires_grad设置成true。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm(X,gamma,beta,moving_mean,moving_var,eps,momentum):\n",
    "    \"\"\"\n",
    "    X:\n",
    "    输入的数据（比如全连接层输出的，relu层之前）\n",
    "    gamma和beta：\n",
    "    就是可以学习的那两个超参数\n",
    "    moving_mean和moving_var\n",
    "    就是随机偏移和随机缩放，可以认为这是全局的均值和方差，是做推理的时候用的。\n",
    "    可以认为是整个数据集上的均值和方差，而不是某个mini_batch上的均值和方差\n",
    "    eps，\n",
    "    就是为了防止方差为0，防止出现除0错误。如果不加这个简单的东西，可能一切都不同了\n",
    "    一般也有个固定的值（轻易不要去修改）\n",
    "    momentum\n",
    "    是用来更新moving_mean和moving_var的一个东西，通常取0.9或者固定的一个东西\n",
    "    \n",
    "    \"\"\"\n",
    "    # 推理模式\n",
    "    if not torch.is_grad_enabled(): # 如果梯度计算不可用（也就是推理模式）\n",
    "        X_hat=(X-moving_mean)/torch.sqrt(moving_var+eps)  \n",
    "    \n",
    "    # 训练模式\n",
    "    else:\n",
    "        assert len(X.shape) in (2,4) \n",
    "        if len(X.shape)==2:  # 对于全连接层来说，作用在特征维\n",
    "            mean=X.mean(dim=0) # 按照行求均值  全连接层的BN作用在特征维上  最终得到一个1xn的行向量， n表示全连接层输入数据的特征数\n",
    "            var=((X-mean)**2).mean(dim=0)   # 方差=(每个样本-均值)的平方和，再除以样本数，看下面的公式。其实把求和和除以样本数这步，直接用mean方法代替了\n",
    "        \n",
    "        else: # 对于卷积层来说，作用在通道维度(也就是把所有通道对应位置加起来，求平均，得到的应该是 1x通道的高x通道的宽 这么一个矩阵 )\n",
    "            mean=X.mean(dim=(0,2,3),keepdim=True) # 这里最终得到的就是 1xnx1x1的一个4d的结果，n表示输入/输出通道数\n",
    "            var=((X-mean)**2.mean(dim=(0,2,3),keepdim=True))\n",
    "           \n",
    "        X-hat=(X-mean)/torch.sqrt(var+eps)\n",
    "        6:19\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证全连接层的计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推理：\n",
    "+ 推理的时候，对输入数据的BN处理，其实是使用了全局的均值和方差\n",
    "+ 因为推理的时候，可能X就只是一个样本，一个batch就是一个样本，那么这个时候如果还使用batch的话 就没法计算了，所以使用全局的均值和方差\n",
    "+ 推理的时候，使用的全局的均值和方差其实来源于预测集的均值和方差（实际推理的时候，也默认新的数据和预测集分布一致，也可以使用预测集的均值和方差）\n",
    "\n",
    "训练：\n",
    "+ 首先判断是全连接层还是卷积层（只会作用在这两种层上，其它层不适用/没有写对应的处理函数），这里只设计了2d的卷积，其他1d和3d的不支持\n",
    "    + 全连接层的输入维度是：batch_size，每个样本的维度（一维向量），比如：(32,64)表示每个样本是64维，每批是32个样本\n",
    "    + 卷积层的输入维度是：(batch_size,输入通道数，高，宽)\n",
    "+ 如果对axis或者dim有疑惑的话，可以去复习一下之前写的总结，[numpy中关于数组维度的理解——dim和axis](https://blog.csdn.net/Castlehe/article/details/116022827)\n",
    "\n",
    "均值和方差计算公式：\n",
    "+ 参考百度百科，点击[这里](https://baike.baidu.com/item/%E6%96%B9%E5%B7%AE%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F/5318566?fr=aladdin)\n",
    "\n",
    "平均值的计算公式为：\n",
    "$$M=\\frac{x_1+x_2+x_3+...+x_n}{n}$$\n",
    "\n",
    "方差的计算公式为：\n",
    "$$s^2=\\frac{(x_1-M)^2+(x_2-M)^2+(x_3-M)^2+...+(x_n-M)^2}{n}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2182, 0.9085],\n",
      "        [0.3319, 0.3531],\n",
      "        [0.3642, 0.6632],\n",
      "        [0.3902, 0.5621],\n",
      "        [0.3267, 0.0696],\n",
      "        [0.1570, 0.3461],\n",
      "        [0.4564, 1.0000],\n",
      "        [0.5459, 0.1419],\n",
      "        [0.0855, 0.2092],\n",
      "        [0.5688, 0.2629]])\n",
      "tensor([0.3445, 0.4517])\n",
      "tensor([0.0224, 0.0924])\n"
     ]
    }
   ],
   "source": [
    "X=torch.rand((10,2))  # 创建10个样本，每个样本的特征数是2维\n",
    "print(X)\n",
    "mean=X.mean(dim=0)\n",
    "print(mean)   #按照列求均值，最终得到各列特征的均值\n",
    "print(((X-mean)**2).mean(dim=0))  \n",
    "# (X-mean)**2 这个的维度其实和x一样，\n",
    "# 所以后面的mean(dim=0)和求均值得到的结果维度也是一样的。  \n",
    "# 都求的是某列特征的统计指标（均值/方差）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证卷积层的计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0.3377, 0.0040, 0.1068],\n",
      "          [0.8988, 0.8476, 0.4406]],\n",
      "\n",
      "         [[0.2373, 0.8555, 0.5407],\n",
      "          [0.0875, 0.8951, 0.0701]],\n",
      "\n",
      "         [[0.9436, 0.8168, 0.6496],\n",
      "          [0.9063, 0.3542, 0.3073]]],\n",
      "\n",
      "\n",
      "        [[[0.5269, 0.7035, 0.4117],\n",
      "          [0.2906, 0.5506, 0.3359]],\n",
      "\n",
      "         [[0.9111, 0.6993, 0.8829],\n",
      "          [0.9526, 0.2769, 0.9301]],\n",
      "\n",
      "         [[0.2550, 0.0682, 0.5667],\n",
      "          [0.2577, 0.5471, 0.3435]]]])\n",
      "tensor([[[[0.4546]],\n",
      "\n",
      "         [[0.6116]],\n",
      "\n",
      "         [[0.5013]]]]) \n",
      " torch.Size([1, 3, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "X=torch.rand((2,3,2,3)) # 两个三通道的，图像高为2，宽为3的输入\n",
    "print(X)\n",
    "mean=X.mean(dim=(0,2,3),keepdim=True)\n",
    "print(mean,\"\\n\",mean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相当于先把所有batch加一起，然后把每个通道的高加起来，把通道的宽加起来，所有batch的所有通道最后就变成了 一个batch的通道数，比如上面的例子中，最终结果就是通道数个数字。 相当于对所有通道进行mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "逐步拆解一下，X.mean(dim=(0,2,3),keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim=0的mean结果：\n",
      "tensor([[[[0.3743, 0.6863, 0.5956],\n",
      "          [0.7442, 0.5927, 0.2910]],\n",
      "\n",
      "         [[0.5139, 0.9126, 0.3580],\n",
      "          [0.8266, 0.9070, 0.7208]],\n",
      "\n",
      "         [[0.7736, 0.8704, 0.1138],\n",
      "          [0.3995, 0.3889, 0.5261]]]])\n",
      "dim=(0,2)的mean结果：\n",
      "tensor([[[[0.5593, 0.6395, 0.4433]],\n",
      "\n",
      "         [[0.6703, 0.9098, 0.5394]],\n",
      "\n",
      "         [[0.5865, 0.6297, 0.3200]]]])\n",
      "对dim_0_mean进行dim=(2)的mean结果：\n",
      "tensor([[[[0.5593, 0.6395, 0.4433]],\n",
      "\n",
      "         [[0.6703, 0.9098, 0.5394]],\n",
      "\n",
      "         [[0.5865, 0.6297, 0.3200]]]])\n"
     ]
    }
   ],
   "source": [
    "dim_0_mean=X.mean(dim=0,keepdim=True)\n",
    "print(f\"dim=0的mean结果：\\n{dim_0_mean}\")  # 也可以把keepdim去掉看看 就是多一层括号而已，不影响什么\n",
    "\n",
    "\n",
    "dim_0_2_mean=X.mean(dim=(0,2),keepdim=True)\n",
    "dim_02_mean=dim_0_mean.mean(dim=(2),keepdim=True)\n",
    "print(f\"dim=(0,2)的mean结果：\\n{dim_0_2_mean}\")\n",
    "print(f\"对dim_0_mean进行dim=(2)的mean结果：\\n{dim_02_mean}\")  # 可以看到，结果是一样的，所以就是逐步对每个axis进行mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 简洁实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
