{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💌**假设我们想对函数$y=2\\textbf{x}^T\\textbf{x}$关于列向量$\\textbf{x}$求导**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x=torch.arange(4.0)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🥗**在计算y关于$\\textbf{x}$的梯度之前，需要一个地方存储梯度**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.requires_grad_(True)\n",
    "# 这里就是表明 需要把x的梯度存下来\n",
    "# 注意这里是requires_grad_，最后有个横杠，这个是函数\n",
    "# 不是requires_grad ，这个是属性/实例\n",
    "# 等价于 x=torch.arange(4.0，requires_grad=True)\n",
    "x.grad \n",
    "# 默认是None，还没有计算过"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后计算y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(28., grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=2*torch.dot(x,x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🍤**通过调用反向传播函数来自动计算`y`关于`x`每个分量的梯度**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.,  4.,  8., 12.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里可以自己算一下，$y=2x^2$，y对x的导数就是$4x$，看看x.grad和x向量的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([nan, 4., 4., 4.], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad/x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad==4*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🍔**现在计算x另一个函数，求另一波导数**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 默认情况下，pytorch会累计梯度，\n",
    "# 所以针对同一个变量求导时，需要清楚之前的值\n",
    "\n",
    "x.grad.zero_() # 把x的梯度信息清零\n",
    "\n",
    "y=x.sum()\n",
    "# 新的函数关系，这个条件下，求y对x的导数\n",
    "y.backward()\n",
    "x.grad\n",
    "# x.grad返回的是代入x值之后的导数，\n",
    "# 这里x.sum() y对x的导数就是常量，相当于dy/dx=x/x=1\n",
    "\n",
    "# 可以看一下注释掉x.grad.zero_()这行之后是什么样子，梯度会累计相加"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🤡**深度学习中，目的不是计算微分矩阵，而是`批量中`每个样本`单独计算`的`偏导数之和`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 2., 4., 6.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对非标量调用`backward`需要传入一个`gradient`参数，\n",
    "# 该参数指定微分函数\n",
    "x.grad.zero_()\n",
    "# 老规矩，先清零\n",
    "\n",
    "y=x*x\n",
    "\n",
    "\n",
    "y.sum().backward()\n",
    "# 等价于y.backward(torch.ones(len(x)))\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 前面两个例子y都是标量，一个数值\n",
    "+ 这里给的y是向量 所以理论上y对x的梯度是个矩阵\n",
    "+ 但是深度学习很少做这样的事，很少对一个向量的函数求导，所以大部分情况下，会对y做一个求和，把y变成标量，然后对标量进行求导\n",
    "+ 不是所有情况都会求和，但是要想办法把它变成一个还可以的标量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad==2*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y=x^2$这里求y对x的导数，结果就是$\\frac{dy}{dx}=2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🎂**将某些计算移动到记录的计算图之外**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "\n",
    "y=x*x\n",
    "u=y.detach()\n",
    "\n",
    "# Returns a new Tensor, detached from the current graph.\n",
    "# The result will never require gradient.\n",
    "z=u*x\n",
    "\n",
    "z.sum().backward()\n",
    "x.grad==u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 1., 2., 3.], requires_grad=True),\n",
       " tensor([0., 1., 4., 9.]),\n",
       " tensor([0., 1., 4., 9.], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,u,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ detach函数会返回一个新张量，这个张量从当前计算图中分离，\n",
    "+ 同时这个结果不会要求梯度计算\n",
    "+ 对y进行detach（），赋值给u，那么u就是y去除是x函数这一属性之后的结果，或者可以理解为y不再是x的函数，而是一个常量，u就不是x的函数了\n",
    "+ 可以看到打印结果，x是需要计算梯度的，u是没有梯度的，所以不再有函数表达，直接代入值，只有计算结果\n",
    "+ 而虽然是对y进行detach操作，但是并没有对y造成影响，依然是梯度函数，grad_fn，只是对这个操作生成的新的张量有影响\n",
    "+ $z=u*x$，所以z关于x的导数$\\frac{dux}{dx}=u$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True, True])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad.zero_()\n",
    "y.sum().backward()\n",
    "x.grad==2*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然u没有梯度，但是并不对y有影响，y依然是x的函数，可以计算梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🍱**即使构建函数的计算图需要通过python控制流（例如：条件、循环或任意函数调用），仍然可以计算得到变量的梯度**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(a):\n",
    "    b=a**2\n",
    "    while b.norm()<1000:\n",
    "        b=b*2\n",
    "    if b.sum()>0:\n",
    "        c=b\n",
    "    else:\n",
    "        c=100*b\n",
    "    return c\n",
    "\n",
    "a=torch.randn(size=(),requires_grad=True)\n",
    "# size等于空，也就是生成一个常量\n",
    "d=f(a)\n",
    "# d对a求导，函数就是f，但是这个函数关系中含有控制流，\n",
    "d.backward()\n",
    "# 但是这里依然可以求导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`b.norm()`这个是求范数，\n",
    "+ 如果b是向量，则就是L2范数，平方求和再开根号\n",
    "+ 如果b是矩阵，则就是F范数，也是平方求和再开根号\n",
    "\n",
    "这里使用隐式构造而不是显式构造导数，这样可以计算复杂的流，但是计算会比显式构造要慢一些\n",
    "\n",
    "+ 显式构造是先把整个计算构造出来，再去给值，简单来说，就是用python实现一个函数和使用数学实现一个函数是不一样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.8705, requires_grad=True), tensor(1915.3682))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.4987, grad_fn=<PowBackward0>)\n",
      "tensor(3.4987, grad_fn=<CopyBackwards>)\n",
      "tensor(3.4987, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 可以逐步验证一下控制流的走向\n",
    "b=a**2\n",
    "print(b)\n",
    "# 这里b设置了一个while循环，要一直b对自己求平方，直到>1000才可以继续\n",
    "print(b.norm())\n",
    "# 标量的norm等于本身的绝对值\n",
    "print(b.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0096]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c=torch.rand(size=(1,1))\n",
    "c\n",
    "# 关于size参数， \n",
    "\"\"\"\n",
    "size (int...): a sequence of integers defining the shape of the output tensor.\n",
    "Can be a variable number of arguments or a collection like a list or tuple.\n",
    "\n",
    "一个用于定义输出向量形状的整数序列\n",
    "可以是一个值可变的变量，或者是类似list或者tuple的集合\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
